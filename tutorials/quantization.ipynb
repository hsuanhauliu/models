{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practicing Post-Training Static Quantization\n",
    "\n",
    "This notebook provides a practical example of **post-training static quantization**, a technique used to make neural network models smaller and faster. Quantization works by converting the model's weights and activations from 32-bit floating-point numbers to lower-precision integers, typically 8-bit.\n",
    "\n",
    "**Benefits:**\n",
    "- **Reduced Model Size**: Smaller models require less storage and are easier to deploy.\n",
    "- **Faster Inference**: Integer arithmetic is much faster than floating-point arithmetic on most hardware, especially on CPUs and edge devices.\n",
    "- **Lower Power Consumption**: Faster calculations lead to reduced energy usage.\n",
    "\n",
    "We will use:\n",
    "- **Model**: A pre-trained `MobileNetV2` from `torchvision`.\n",
    "- **Dataset**: `CIFAR-10`.\n",
    "- **Technique**: Post-Training Static Quantization in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup\n",
    "\n",
    "First, we'll import the necessary libraries. Note that for quantization, we'll be working primarily on the CPU, as that's where the performance benefits of quantized integer models are most pronounced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.quantization import quantize_dynamic, prepare, convert\n",
    "import os\n",
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "# Set device to CPU for quantization\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data and Pre-trained Model\n",
    "\n",
    "We will load the CIFAR-10 dataset and a pre-trained MobileNetV2 model. We'll also define a helper function to evaluate the model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 test dataset\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)\n",
    "\n",
    "# Helper function for evaluation\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total\n",
    "\n",
    "# Load a pre-trained MobileNetV2 model\n",
    "original_model = torchvision.models.mobilenet_v2(weights=torchvision.models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "# Modify the classifier for CIFAR-10\n",
    "original_model.classifier[1] = nn.Linear(original_model.last_channel, 10)\n",
    "original_model.to(device)\n",
    "original_model.eval()\n",
    "\n",
    "print(\"Loaded pre-trained MobileNetV2 model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare the Model for Quantization\n",
    "\n",
    "Post-training static quantization involves a few preparation steps:\n",
    "1.  **Fuse Modules**: We merge operations like Convolution -> BatchNorm -> ReLU into a single, more efficient block. This is crucial for quantization accuracy and speed.\n",
    "2.  **Specify Quantization Config**: We define how the weights and activations should be quantized. `fbgemm` is a standard backend for x86 CPUs.\n",
    "3.  **Prepare the Model**: We insert \"observer\" modules into the model. These observers will watch the range of activation values during a calibration step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = deepcopy(original_model)\n",
    "quantized_model.eval()\n",
    "\n",
    "# Fuse Conv, BatchNorm, and ReLU modules\n",
    "torch.quantization.fuse_modules(quantized_model.features, [['0', '1'], ['2', '3'], ['4', '5'], ['7', '8'], ['9', '10'], ['11', '12'], ['13', '14'], ['16', '17']], inplace=True)\n",
    "\n",
    "# Specify quantization configuration\n",
    "# 'fbgemm' is a backend for x86 CPUs\n",
    "quantized_model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "\n",
    "# Prepare the model for static quantization. This inserts observers in the model\n",
    "torch.quantization.prepare(quantized_model, inplace=True)\n",
    "\n",
    "print(\"Model prepared for quantization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calibrate the Model\n",
    "\n",
    "Now we need to run a small amount of representative data through the prepared model. The observers we inserted will record the distribution of the activations. This information is then used to determine the optimal quantization parameters (scale and zero-point) for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calibrating the model with a subset of the test data...\")\n",
    "# Use a subset of the test loader for calibration\n",
    "calibration_loader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, _ in calibration_loader:\n",
    "        quantized_model(images)\n",
    "\n",
    "print(\"Calibration complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Convert to a Quantized Model\n",
    "\n",
    "With the calibration data collected, we can now convert the model. This step replaces the floating-point operations with their integer-based, quantized equivalents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Converting the model to a quantized version...\")\n",
    "torch.quantization.convert(quantized_model, inplace=True)\n",
    "print(\"Model converted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare and Evaluate\n",
    "\n",
    "Now for the moment of truth. Let's compare the original floating-point model with our new quantized integer model in three key areas: model size, accuracy, and inference speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Compare Model Size ---\n",
    "def print_model_size(model, label):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    size = os.path.getsize(\"temp.p\") / 1e6 # size in MB\n",
    "    print(f\"Size of {label}: {size:.2f} MB\")\n",
    "    os.remove(\"temp.p\")\n",
    "\n",
    "print_model_size(original_model, \"Original Model\")\n",
    "print_model_size(quantized_model, \"Quantized Model\")\n",
    "\n",
    "# --- 2. Compare Accuracy ---\n",
    "# NOTE: The original model is not fine-tuned on CIFAR-10, so accuracy will be low.\n",
    "# The key is to observe the *drop* in accuracy after quantization.\n",
    "print(\"\\nEvaluating accuracy...\")\n",
    "original_accuracy = evaluate_model(original_model, testloader)\n",
    "quantized_accuracy = evaluate_model(quantized_model, testloader)\n",
    "\n",
    "print(f\"Accuracy of Original Model: {original_accuracy:.2f}%\")\n",
    "print(f\"Accuracy of Quantized Model: {quantized_accuracy:.2f}%\")\n",
    "print(f\"Accuracy Drop: {original_accuracy - quantized_accuracy:.2f}%\")\n",
    "\n",
    "# --- 3. Compare Inference Speed ---\n",
    "def time_inference(model, dataloader):\n",
    "    model.eval()\n",
    "    latencies = []\n",
    "    with torch.no_grad():\n",
    "        for images, _ in dataloader:\n",
    "            start = time.time()\n",
    "            _ = model(images)\n",
    "            end = time.time()\n",
    "            latencies.append(end - start)\n",
    "    return sum(latencies) / len(latencies)\n",
    "\n",
    "print(\"\\nComparing inference speed...\")\n",
    "original_latency = time_inference(original_model, testloader)\n",
    "quantized_latency = time_inference(quantized_model, testloader)\n",
    "\n",
    "print(f\"Average latency of Original Model: {original_latency * 1000:.2f} ms\")\n",
    "print(f\"Average latency of Quantized Model: {quantized_latency * 1000:.2f} ms\")\n",
    "print(f\"Speedup: {original_latency / quantized_latency:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "As you can see from the results, post-training static quantization dramatically reduces the model size (typically by about 4x) and can significantly speed up inference on the CPU. The trade-off is usually a small drop in accuracy. For many applications, especially on resource-constrained devices, this trade-off is highly favorable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
