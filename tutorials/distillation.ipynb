{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Distillation for Vision Models\n",
    "\n",
    "**Model distillation** is a knowledge transfer technique where a small \"student\" model learns from a larger, pre-trained \"teacher\" model. The goal is to train a compact and fast student model that mimics the performance of the much larger teacher.\n",
    "\n",
    "We will use:\n",
    "- **Teacher Model**: A pre-trained `ResNet-18` from `torchvision`.\n",
    "- **Student Model**: A simple, custom-built Convolutional Neural Network (CNN).\n",
    "- **Dataset**: `CIFAR-10`.\n",
    "- **Monitoring**: `TensorBoard` for logging training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-13 04:13:43.379525: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1760328823.392143   20482 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1760328823.396011   20482 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-10-13 04:13:43.408323: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Student Model\n",
    "\n",
    "This is our small, lightweight model that we want to train. It's a simple CNN, much less complex than our ResNet-18 teacher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StudentNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 128)\n",
    "        self.fc2 = nn.Linear(128, 10) # 10 classes for CIFAR-10\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 8 * 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Data and Models\n",
    "\n",
    "We'll load the CIFAR-10 dataset and apply standard transformations. Then, we'll load our pre-trained ResNet-18 teacher model and instantiate our StudentNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)\n",
    "\n",
    "# Load the pre-trained teacher model (ResNet-18)\n",
    "teacher_model = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "# Modify the final layer for CIFAR-10 (10 classes)\n",
    "num_ftrs = teacher_model.fc.in_features\n",
    "teacher_model.fc = nn.Linear(num_ftrs, 10)\n",
    "teacher_model = teacher_model.to(device)\n",
    "# For this example, we'll assume the teacher is already trained on CIFAR-10.\n",
    "# In a real scenario, you would fine-tune this teacher model on CIFAR-10 first.\n",
    "teacher_model.eval() # Set teacher to evaluation mode\n",
    "\n",
    "# Instantiate the student model\n",
    "student_model = StudentNet().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Distillation Loss Function\n",
    "\n",
    "The loss is a weighted average of two components:\n",
    "\n",
    "1.  **Student Loss**: The standard cross-entropy loss between the student's predictions and the true labels. This teaches the student to predict the correct class.\n",
    "2.  **Distillation Loss**: The Kullback-Leibler (KL) Divergence loss between the softened outputs of the teacher and the student. This encourages the student to mimic the teacher's probability distribution over classes, learning not just *what* the correct class is, but also *how* the teacher relates different classes to each other.\n",
    "\n",
    "The `temperature` parameter is used to soften the probability distributions, making them less peaked and providing more information for the student to learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distillation_loss(student_logits, teacher_logits, true_labels, temperature, alpha):\n",
    "    \"\"\"\n",
    "    Calculates the distillation loss.\n",
    "    \n",
    "    Args:\n",
    "        student_logits: Logits from the student model.\n",
    "        teacher_logits: Logits from the teacher model.\n",
    "        true_labels: The ground truth labels.\n",
    "        temperature (float): Softening parameter for the probability distributions.\n",
    "        alpha (float): Weight for the distillation loss component.\n",
    "    \"\"\"\n",
    "    # Distillation loss (KL divergence)\n",
    "    soft_teacher_probs = F.softmax(teacher_logits / temperature, dim=1)\n",
    "    soft_student_log_probs = F.log_softmax(student_logits / temperature, dim=1)\n",
    "    distill_loss = F.kl_div(soft_student_log_probs, soft_teacher_probs, reduction='batchmean') * (temperature ** 2)\n",
    "\n",
    "    # Standard cross-entropy loss with true labels\n",
    "    student_loss = F.cross_entropy(student_logits, true_labels)\n",
    "\n",
    "    # Combine the two losses\n",
    "    total_loss = alpha * distill_loss + (1. - alpha) * student_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Student Model\n",
    "\n",
    "Now we'll write the training loop. In each step, we get predictions from both the teacher and the student. We then calculate our custom `distillation_loss` and update the student's weights. The teacher's weights remain frozen throughout.\n",
    "\n",
    "We will also evaluate the model on the test set at the end of each epoch and log the results to TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting distillation training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 391/391 [00:03<00:00, 111.18it/s, loss=0.65] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Test Accuracy: 53.03 %\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15: 100%|██████████| 391/391 [00:03<00:00, 118.96it/s, loss=0.611]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Test Accuracy: 58.94 %\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15: 100%|██████████| 391/391 [00:03<00:00, 118.58it/s, loss=0.589]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Test Accuracy: 62.85 %\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15: 100%|██████████| 391/391 [00:03<00:00, 120.05it/s, loss=0.579]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Test Accuracy: 64.61 %\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15: 100%|██████████| 391/391 [00:03<00:00, 116.00it/s, loss=0.561]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 Test Accuracy: 66.03 %\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15: 100%|██████████| 391/391 [00:03<00:00, 117.47it/s, loss=0.546]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 Test Accuracy: 65.93 %\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15: 100%|██████████| 391/391 [00:03<00:00, 118.77it/s, loss=0.54] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 Test Accuracy: 66.95 %\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15: 100%|██████████| 391/391 [00:03<00:00, 119.14it/s, loss=0.547]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 Test Accuracy: 66.79 %\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15: 100%|██████████| 391/391 [00:03<00:00, 117.78it/s, loss=0.528]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 Test Accuracy: 67.99 %\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15: 100%|██████████| 391/391 [00:03<00:00, 117.72it/s, loss=0.536]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 Test Accuracy: 66.63 %\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15: 100%|██████████| 391/391 [00:03<00:00, 120.18it/s, loss=0.531]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11 Test Accuracy: 68.56 %\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15: 100%|██████████| 391/391 [00:03<00:00, 118.65it/s, loss=0.532]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12 Test Accuracy: 68.83 %\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15: 100%|██████████| 391/391 [00:03<00:00, 117.09it/s, loss=0.518]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13 Test Accuracy: 68.24 %\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15: 100%|██████████| 391/391 [00:03<00:00, 118.12it/s, loss=0.524]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14 Test Accuracy: 69.71 %\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15: 100%|██████████| 391/391 [00:03<00:00, 117.65it/s, loss=0.508]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15 Test Accuracy: 68.72 %\n",
      "\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "epochs = 15\n",
    "learning_rate = 0.001\n",
    "temperature = 5.0\n",
    "alpha = 0.7 # Weight for distillation loss\n",
    "\n",
    "optimizer = optim.Adam(student_model.parameters(), lr=learning_rate)\n",
    "writer = SummaryWriter('./logs/distillation') # TensorBoard writer\n",
    "\n",
    "print(\"Starting distillation training...\")\n",
    "global_step = 0\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    student_model.train() # Set student to training mode\n",
    "    progress_bar = tqdm(trainloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for inputs, labels in progress_bar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass for the student\n",
    "        student_logits = student_model(inputs)\n",
    "\n",
    "        # Forward pass for the teacher (no gradient calculation needed)\n",
    "        with torch.no_grad():\n",
    "            teacher_logits = teacher_model(inputs)\n",
    "\n",
    "        # Calculate the distillation loss\n",
    "        loss = distillation_loss(student_logits, teacher_logits, labels, temperature, alpha)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': running_loss / (progress_bar.n + 1)})\n",
    "        \n",
    "        # Log training loss to TensorBoard\n",
    "        writer.add_scalar('Loss/train_step', loss.item(), global_step)\n",
    "        global_step += 1\n",
    "    \n",
    "    # --- Evaluation at the end of each epoch ---\n",
    "    student_model.eval() # Set student to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = student_model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'\\nEpoch {epoch+1} Test Accuracy: {accuracy:.2f} %\\n')\n",
    "    \n",
    "    # Log accuracy and epoch loss to TensorBoard\n",
    "    writer.add_scalar('Accuracy/test', accuracy, epoch)\n",
    "    writer.add_scalar('Loss/train_epoch', running_loss / len(trainloader), epoch)\n",
    "\n",
    "\n",
    "writer.close()\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d81bb64438576b66\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d81bb64438576b66\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs --port 6006 --bind_all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
